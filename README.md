Retail Shop (E-commerce Microservices) â€“ Documentation & Terraform Pack
This pack includes a productionâ€‘ready README.md, supporting docs, a refactored Terraform module for EKS, Kubernetes manifests (nonâ€‘Helm), and a CI/CD workflow for GitHub Actions.

---

ğŸ“ Repository Structure (proposed)
.
â”œâ”€ README.md
â”œâ”€ terraform/
â”‚ â””â”€ main.tf
â”œâ”€ k8s/
â”‚ â”œâ”€ catalog-deployment.yaml
â”‚ â”œâ”€ catalog-service.yaml
â”‚ â”œâ”€ ui-deployment.yaml
â”‚ â”œâ”€ ui-service.yaml
â””â”€ .github/
â””â”€ workflows/
â””â”€ docker-image.yml

If your current repo already has some of these, merge or replace as needed. Secrets used by CI/CD must be created in the repoâ€™s Settings â†’ Secrets and variables â†’ Actions.

---

1. README.md

# Retail Shop â€“ Microservices Eâ€‘commerce (DevOps Assignment)

This repository contains a containerized microservice (e.g., `product-service`) deployed on **Amazon EKS** using **Kubernetes manifests** (no Helm for app deployment), with **CI/CD via GitHub Actions**, and **monitoring using Prometheus & Grafana**. Infrastructure is provisioned using **Terraform**.

## âš™ï¸ Tech Stack

- **Containerization:** Docker â†’ Docker Hub registry
- **IaC:** Terraform â†’ Amazon EKS + VPC + managed node group
- **Orchestration:** Kubernetes (manifests)
- **CI/CD:** GitHub Actions (build, test, push image, deploy to EKS)
- **Observability:** Prometheus + Grafana (dashboards for uptime & request metrics)

## ğŸ§­ Architecture Overview

Dev â†’ GitHub â†’ GitHub Actions â†’ Docker Hub â†’ EKS (K8s) â†’ Users â†‘ â†• Terraform (EKS) Prometheus/Grafana

### Main Components

- **EKS Cluster** with two public subnets and a managed node group.
- **product-service** Kubernetes Deployment with resource limits, liveness/readiness probes, and **HPA**.
- **Service** exposed via **Ingress** (NGINX) or **LoadBalancer**.
- **Prometheus/Grafana** used to monitor uptime and request metrics.

## ğŸš€ Quick Start

### A) Infrastructure (Terraform)

```bash
cd infra/terraform
terraform init
terraform plan -out tfplan
terraform apply tfplan
Outputs include VPC ID, EKS cluster endpoint/name, node group, and public subnets. After apply, update kubeconfig:
aws eks update-kubeconfig --region <region> --name <cluster_name>
B) Build & Push Image (local)
docker build -t ${{ secrets.DOCKER_HUB_USERNAME }}/ui:${{ github.sha }} ./ui docker push ${{ secrets.DOCKER_HUB_USERNAME }}/ui:${{ github.sha }}
C) Deploy to Kubernetes (manifests)
kubectl apply -f k8s/catalog-deployment.yaml
kubectl apply -f k8s/catalog-service.yaml
kubectl apply -f k8s/ui-deployment.yaml
kubectl apply -f k8s/ui-service.yaml
(or)
kubectl apply -f .
D) CI/CD (GitHub Actions)
On push to main, the workflow will build & push the Docker image and apply manifests to EKS. Configure GitHub Action secrets (see docs/CI-CD.md).
E) Monitoring
â€¢	Install Prometheus & Grafana (see docs/MONITORING.md).
â€¢	Service is annotated for scraping basic HTTP metrics (examples provided). Import the provided dashboard JSON or build panels using PromQL queries.
ğŸ” Secrets & Config
â€¢	Put nonâ€‘secret runtime config into k8s/configmap.yaml.
â€¢	Store secrets in GitHub Actions Secrets (CI/CD) and Kubernetes Secrets (runtime) if needed.
âœ… Security, Scalability, Fault Tolerance
â€¢	Limited blast radius via VPC networking; managed node groups.
â€¢	Resource requests/limits + HPA for elasticity.
â€¢	Readiness/liveness probes for selfâ€‘healing.
â€¢	Network access controlled by Security Groups; use private subnets + NAT for production.
ğŸ“„ Useful Paths
â€¢	Terraform: terraform/
â€¢	Kubernetes Manifests: k8s/
â€¢	CI/CD Workflow: .github/workflows/ docker-image.yml
â€¢	Docs: docs/
ğŸ§¹ Cleanup
cd infra/terraform
terraform destroy

## âœ… Security, Scalability, Fault Tolerance

- **Networking isolation:** Dedicated VPC, subnets, and security groups for the EKS cluster.
- **Scalability:** Horizontal Pod Autoscaler (HPA) defined; scales pods based on CPU utilization (requires metrics-server).
- **Self-healing:** Liveness and readiness probes ensure unhealthy pods are restarted and traffic is routed only to healthy pods.
- **Access control:** Security Group rules restrict access; currently broad for demo purposes. In a production setup, private subnets with a NAT gateway would be recommended.

________________________________________
2) docs/ARCHITECTURE.md
Architecture
Highâ€‘level
â€¢	Microservice (e.g., product-service) containerized with Docker, pushed to Docker Hub.
â€¢	EKS cluster hosts the app. Traffic â†’ Ingress â†’ Service â†’ Pods.
â€¢	Prometheus scrapes targets and Grafana visualizes metrics.
Kubernetes Objects
â€¢	Namespace â†’ logical isolation.
â€¢	Deployment â†’ declarative rollout, replicas.
â€¢	Service (ClusterIP) â†’ stable access to pods.
â€¢	Ingress (NGINX) â†’ external HTTP(S) routing.
â€¢	HPA â†’ scale pods based on CPU (or custom metrics if available).
Reliability & Scaling
â€¢	Rolling updates with maxUnavailable/maxSurge.
â€¢	Probes for health checks.
â€¢	Requests/limits tuned for HPA.
________________________________________
3) docs/ASSIGNMENT_MAPPING.md
Assignment â†’ Implementation Mapping
â€¢	Containerization â†’ Dockerfile and CI build pushing to Docker Hub.
â€¢	IaC (Terraform) â†’ EKS cluster, VPC, subnets, security groups, IAM roles/policies.
â€¢	Kubernetes Deployment â†’ Plain manifests (no Helm) for Deployment
â€¢	CI/CD â†’ GitHub Actions builds, tests, pushes image, then applies manifests on main.
â€¢	Monitoring & Logging â†’ Prometheus & Grafana; dashboards for uptime & request metrics.
Deliverables are organized under repo root (README.md), infra/terraform, k8s, .github/workflows, and docs/.
________________________________________
4) docs/CI-CD.md
CI/CD â€“ GitHub Actions
Secrets to add (Repository â†’ Settings â†’ Secrets and variables â†’ Actions)
â€¢	DOCKERHUB_USERNAME
â€¢	DOCKERHUB_TOKEN
â€¢	AWS_ACCESS_KEY_ID
â€¢	AWS_SECRET_ACCESS_KEY
â€¢	KUBECONFIG_CONTENT

Workflow Summary
1.	Checkout
2.	Log in to Docker Hub
3.	Build & push Docker image
4.	Configure AWS creds
5.	Update kubeconfig for EKS
6.	Apply Kubernetes manifests
The workflow file is in .github/workflows/ci-cd.yaml.
________________________________________
5) docs/MONITORING.md
Monitoring â€“ Prometheus & Grafana
Option A (recommended): kubeâ€‘prometheusâ€‘stack (Helm)
â€¢	Installs Prometheus Operator, CRDs, Alertmanager, Grafana, and exporters.
â€¢	After install, expose Grafana via Service or Ingress.
Option B: Minimal Prometheus + Grafana (manifests)
â€¢	Use standalone Deployments/Services for Prometheus and Grafana.
â€¢	Annotate application Service for HTTP metrics scraping.
Scrape Annotations (example)
Your k8s/service.yaml is annotated to scrape /metrics on port 8080. If your app exposes metrics elsewhere, adjust annotations.
Key PromQL
â€¢	Uptime (per pod):
 	max by (pod) (time() - process_start_time_seconds{job="product-service"})
â€¢	Request rate (HTTP):
 	sum(rate(http_requests_total{app="product-service"}[5m]))
â€¢	Error rate (5xx):
 	sum(rate(http_requests_total{app="product-service",status=~"5.."}[5m]))
Grafana
â€¢	Import a simple dashboard with 3 panels: Uptime, Request Rate, Error Rate.
â€¢	Add Prometheus as a data source: URL http://prometheus-server.prometheus.svc.cluster.local:80 (adjust namespace/service name to your install).
________________________________________

```
